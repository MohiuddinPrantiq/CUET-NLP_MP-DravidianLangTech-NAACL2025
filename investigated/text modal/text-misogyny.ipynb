{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10523956,"sourceType":"datasetVersion","datasetId":6404969}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import precision_recall_fscore_support, classification_report\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom transformers import (AutoTokenizer, AutoModelForSequenceClassification, \n                        TrainingArguments, Trainer)\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\nfrom tabulate import tabulate\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-25T19:58:27.869073Z","iopub.execute_input":"2025-01-25T19:58:27.869486Z","iopub.status.idle":"2025-01-25T19:58:50.048857Z","shell.execute_reply.started":"2025-01-25T19:58:27.869453Z","shell.execute_reply":"2025-01-25T19:58:50.047880Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import wandb\nimport os\nwandb.login(key='02f62ea18d807de380f948a102283d73ce32d0ef')\n\nos.environ[\"WANDB_MODE\"] = \"online\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T19:58:50.050048Z","iopub.execute_input":"2025-01-25T19:58:50.050898Z","iopub.status.idle":"2025-01-25T19:58:58.198734Z","shell.execute_reply.started":"2025-01-25T19:58:50.050873Z","shell.execute_reply":"2025-01-25T19:58:58.197880Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohiuddinprantiq\u001b[0m (\u001b[33mmohiuddinprantiq-chittagong-university-of-engineering-an\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def load_data(train_path, val_path, test_path):\n    \"\"\"Load train, validation and test data\"\"\"\n    train_df = pd.read_csv(train_path)\n    val_df = pd.read_csv(val_path)\n    test_df = pd.read_csv(test_path)\n    \n    return (train_df['transcriptions'].values, train_df['labels'].values,\n            val_df['transcriptions'].values, val_df['labels'].values,\n            test_df['transcriptions'].values, test_df['labels'].values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T19:58:58.200622Z","iopub.execute_input":"2025-01-25T19:58:58.201124Z","iopub.status.idle":"2025-01-25T19:58:58.205239Z","shell.execute_reply.started":"2025-01-25T19:58:58.201101Z","shell.execute_reply":"2025-01-25T19:58:58.204378Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def evaluate_metrics(y_true, y_pred):\n    \"\"\"Calculate precision, recall, and f1-score\"\"\"\n    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n    return {\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T19:58:58.206393Z","iopub.execute_input":"2025-01-25T19:58:58.206699Z","iopub.status.idle":"2025-01-25T19:58:58.222996Z","shell.execute_reply.started":"2025-01-25T19:58:58.206678Z","shell.execute_reply":"2025-01-25T19:58:58.222195Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# dataset for transformer\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        \n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T19:58:58.223705Z","iopub.execute_input":"2025-01-25T19:58:58.223946Z","iopub.status.idle":"2025-01-25T19:58:58.232910Z","shell.execute_reply.started":"2025-01-25T19:58:58.223926Z","shell.execute_reply":"2025-01-25T19:58:58.232220Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# dataset for CNN, BiLSTM\nclass CustomTextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=100):\n        self.texts = texts\n        self.labels = labels\n        sequences = tokenizer.texts_to_sequences(texts)\n        self.data = torch.tensor(pad_sequences(sequences, maxlen=max_len, padding='post'))\n        self.labels = torch.tensor(labels, dtype=torch.long)\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T19:58:58.233619Z","iopub.execute_input":"2025-01-25T19:58:58.233886Z","iopub.status.idle":"2025-01-25T19:58:58.250628Z","shell.execute_reply.started":"2025-01-25T19:58:58.233859Z","shell.execute_reply":"2025-01-25T19:58:58.249855Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class BiLSTM(nn.Module):\n    def __init__(self, vocab_size=10000, embedding_dim=100, hidden_dim=256, output_dim=2):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n        self.dropout = nn.Dropout(0.5)\n    \n    def forward(self, text):\n        embedded = self.embedding(text)\n        output, (hidden, cell) = self.lstm(embedded)\n        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n        return self.fc(self.dropout(hidden))\n\nclass TextCNN(nn.Module):\n    def __init__(self, vocab_size=10000, embedding_dim=100, n_filters=100, filter_sizes=[3,4,5], output_dim=2):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.convs = nn.ModuleList([\n            nn.Conv2d(in_channels=1, out_channels=n_filters, \n                     kernel_size=(fs, embedding_dim)) \n            for fs in filter_sizes\n        ])\n        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n        self.dropout = nn.Dropout(0.5)\n    \n    def forward(self, text):\n        embedded = self.embedding(text)\n        embedded = embedded.unsqueeze(1)\n        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n        cat = self.dropout(torch.cat(pooled, dim=1))\n        return self.fc(cat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T19:58:58.251388Z","iopub.execute_input":"2025-01-25T19:58:58.251693Z","iopub.status.idle":"2025-01-25T19:58:58.265101Z","shell.execute_reply.started":"2025-01-25T19:58:58.251664Z","shell.execute_reply":"2025-01-25T19:58:58.264259Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=5):\n    best_val_loss = float('inf')\n    best_model = None\n    \n    for epoch in range(epochs):\n        # Training phase\n        model.train()\n        total_loss = 0\n        for texts, labels in train_loader:\n            texts, labels = texts.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            predictions = model(texts)\n            loss = criterion(predictions, labels)\n            \n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for texts, labels in val_loader:\n                texts, labels = texts.to(device), labels.to(device)\n                \n                predictions = model(texts)\n                loss = criterion(predictions, labels)\n                val_loss += loss.item()\n                \n                _, predicted = torch.max(predictions, 1)\n                val_total += labels.size(0)\n                val_correct += (predicted == labels).sum().item()\n        \n        val_accuracy = val_correct / val_total\n        val_loss = val_loss / len(val_loader)\n        \n        print(f'Epoch: {epoch+1}')\n        print(f'\\tTrain Loss: {total_loss/len(train_loader):.4f}')\n        print(f'\\tVal Loss: {val_loss:.4f}')\n        print(f'\\tVal Accuracy: {val_accuracy:.4f}')\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_model = model.state_dict().copy()\n    \n    # Load best model\n    model.load_state_dict(best_model)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T19:58:58.265893Z","iopub.execute_input":"2025-01-25T19:58:58.266364Z","iopub.status.idle":"2025-01-25T19:58:58.282419Z","shell.execute_reply.started":"2025-01-25T19:58:58.266334Z","shell.execute_reply":"2025-01-25T19:58:58.281739Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def evaluate_model(model, test_loader, device):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for texts, labels in test_loader:\n            texts, labels = texts.to(device), labels.to(device)\n            \n            predictions = model(texts)\n            _, predicted = torch.max(predictions, 1)\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    return evaluate_metrics(np.array(all_labels), np.array(all_preds))\n\ndef run_cnn_bilstm(X_train, X_val, X_test, y_train, y_val, y_test, model_type='cnn'):\n    # Initialize tokenizer\n    tokenizer = Tokenizer(num_words=10000)\n    tokenizer.fit_on_texts(X_train)\n    vocab_size = len(tokenizer.word_index) + 1\n    \n    # Create datasets\n    batch_size = 32\n    train_dataset = CustomTextDataset(X_train, y_train, tokenizer)\n    val_dataset = CustomTextDataset(X_val, y_val, tokenizer)\n    test_dataset = CustomTextDataset(X_test, y_test, tokenizer)\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n    \n    # Initialize model\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    if model_type == 'cnn':\n        model = TextCNN(vocab_size=vocab_size).to(device)\n        print(\"Training CNN...\")\n    else:\n        model = BiLSTM(vocab_size=vocab_size).to(device)\n        print(\"Training BiLSTM...\")\n    \n    # Training setup\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters())\n    \n    # Train model\n    model = train_model(model, train_loader, val_loader, criterion, optimizer, device)\n    \n    # Evaluate model\n    return evaluate_model(model, test_loader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T19:58:58.284475Z","iopub.execute_input":"2025-01-25T19:58:58.284676Z","iopub.status.idle":"2025-01-25T19:58:58.299233Z","shell.execute_reply.started":"2025-01-25T19:58:58.284658Z","shell.execute_reply":"2025-01-25T19:58:58.298640Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def run_svm(X_train, X_val, X_test, y_train, y_val, y_test):\n    \"\"\"Train and evaluate SVM model\"\"\"\n    print(\"Training SVM...\")\n    vectorizer = TfidfVectorizer(max_features=10000)\n    X_train_tfidf = vectorizer.fit_transform(X_train)\n    X_test_tfidf = vectorizer.transform(X_test)\n    \n    svm = LinearSVC(random_state=42)\n    svm.fit(X_train_tfidf, y_train)\n    y_pred = svm.predict(X_test_tfidf)\n    return evaluate_metrics(y_test, y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T19:58:58.300108Z","iopub.execute_input":"2025-01-25T19:58:58.300314Z","iopub.status.idle":"2025-01-25T19:58:58.315731Z","shell.execute_reply.started":"2025-01-25T19:58:58.300295Z","shell.execute_reply":"2025-01-25T19:58:58.314936Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def train_torch_model(model, train_loader, val_loader, device, epochs=3):\n    \"\"\"Generic training function for PyTorch models\"\"\"\n    optimizer = torch.optim.Adam(model.parameters())\n    criterion = nn.CrossEntropyLoss()\n    model = model.to(device)\n    \n    for epoch in range(epochs):\n        model.train()\n        for batch in train_loader:\n            texts = batch['input_ids'].to(device)\n            labels = batch['labels'].to(device)\n            \n            optimizer.zero_grad()\n            predictions = model(texts)\n            loss = criterion(predictions, labels)\n            loss.backward()\n            optimizer.step()\n    \n    return model\n\ndef evaluate_torch_model(model, test_loader, device):\n    \"\"\"Evaluate PyTorch model\"\"\"\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            texts = batch['input_ids'].to(device)\n            labels = batch['labels']\n            \n            predictions = model(texts)\n            predictions = torch.argmax(predictions, dim=1)\n            \n            all_preds.extend(predictions.cpu().numpy())\n            all_labels.extend(labels.numpy())\n    \n    return evaluate_metrics(all_labels, all_preds)\n\ndef run_transformer_model(model_name, X_train, X_val, X_test, y_train, y_val, y_test):\n    \"\"\"Train and evaluate transformer models\"\"\"\n    print(f\"Training {model_name}...\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n    \n    train_dataset = TextDataset(X_train, y_train, tokenizer)\n    val_dataset = TextDataset(X_val, y_val, tokenizer)\n    test_dataset = TextDataset(X_test, y_test, tokenizer)\n    \n    \n    training_args = TrainingArguments(\n        output_dir=f'./results_{model_name}',\n        num_train_epochs=3,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        evaluation_strategy=\"epoch\",\n        logging_strategy=\"no\",  # Disable logging\n        learning_rate=2e-5,\n        weight_decay=0.01,\n        save_strategy=\"epoch\",\n        save_total_limit=1,  # Keep only the last checkpoint\n        load_best_model_at_end=True,\n    )\n\n\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset\n    )\n    \n    trainer.train()\n    predictions = trainer.predict(test_dataset)\n    y_pred = np.argmax(predictions.predictions, axis=1)\n    return evaluate_metrics(y_test, y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T19:58:58.316524Z","iopub.execute_input":"2025-01-25T19:58:58.316793Z","iopub.status.idle":"2025-01-25T19:58:58.332491Z","shell.execute_reply.started":"2025-01-25T19:58:58.316766Z","shell.execute_reply":"2025-01-25T19:58:58.331720Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def run_all_models(train_path, val_path, test_path):\n    \"\"\"Run all models and compile results\"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Load all data\n    X_train, y_train, X_val, y_val, X_test, y_test = load_data(train_path, val_path, test_path)\n    \n    results = {}\n    \n    # Run SVM\n    results['SVM'] = run_svm(X_train, X_val, X_test, y_train, y_val, y_test)\n    \n    # Run CNN\n    results['CNN'] = run_cnn_bilstm(X_train, X_val, X_test, y_train, y_val, y_test, model_type='cnn')\n    \n    # Run Bi-LSTM\n    results['Bi-LSTM'] = run_cnn_bilstm(X_train, X_val, X_test, y_train, y_val, y_test, model_type='bilstm')\n    \n    # Run transformer models\n    transformer_models = {\n        'XLM-R': 'FacebookAI/xlm-roberta-base',\n        'mBERT': 'google-bert/bert-base-multilingual-cased'\n    }\n    \n    for model_name, model_path in transformer_models.items():\n        results[model_name] = run_transformer_model(\n            model_path, X_train, X_val, X_test, y_train, y_val, y_test\n        )\n    \n    return results\n\ndef display_results(results,lan):\n    \"\"\"Display results in a formatted table\"\"\"\n    rows = []\n    for model_name, metrics in results.items():\n        rows.append([\n            model_name,\n            f\"{metrics['precision']:.4f}\",\n            f\"{metrics['recall']:.4f}\",\n            f\"{metrics['f1']:.4f}\"\n        ])\n    \n    headers = ['Model', 'Precision', 'Recall', 'F1-Score']\n    print(f\"\\nModel Comparison Results for {lan}:\")\n    print(tabulate(rows, headers=headers, tablefmt='grid'))\n    \n    # Save results to CSV\n    results_df = pd.DataFrame(rows, columns=headers)\n    results_df.to_csv(f'model_comparison_results_{lan}.csv', index=False)\n    print(f\"\\nResults saved to 'model_comparison_results_for_{lan}.csv'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T19:58:58.333258Z","iopub.execute_input":"2025-01-25T19:58:58.333530Z","iopub.status.idle":"2025-01-25T19:58:58.349757Z","shell.execute_reply.started":"2025-01-25T19:58:58.333502Z","shell.execute_reply":"2025-01-25T19:58:58.349061Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# tamil\ntrain_path = '/kaggle/input/misogyny/misogyny/misogyny/tamil/train/train.csv'\nval_path = '/kaggle/input/misogyny/misogyny/misogyny/tamil/dev/dev.csv'\ntest_path = '/kaggle/input/misogyny/test_with_labels_tamil/test_with_labels.csv'\nresults_tam = run_all_models(train_path, val_path, test_path)\nlan='tamil'\ndisplay_results(results_tam,lan)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T19:58:58.350381Z","iopub.execute_input":"2025-01-25T19:58:58.350602Z","iopub.status.idle":"2025-01-25T20:04:20.853123Z","shell.execute_reply.started":"2025-01-25T19:58:58.350584Z","shell.execute_reply":"2025-01-25T20:04:20.852464Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nTraining SVM...\nTraining CNN...\nEpoch: 1\n\tTrain Loss: 0.6078\n\tVal Loss: 0.5361\n\tVal Accuracy: 0.7394\nEpoch: 2\n\tTrain Loss: 0.4080\n\tVal Loss: 0.4892\n\tVal Accuracy: 0.7359\nEpoch: 3\n\tTrain Loss: 0.2825\n\tVal Loss: 0.4783\n\tVal Accuracy: 0.7535\nEpoch: 4\n\tTrain Loss: 0.2103\n\tVal Loss: 0.4910\n\tVal Accuracy: 0.7641\nEpoch: 5\n\tTrain Loss: 0.1304\n\tVal Loss: 0.5158\n\tVal Accuracy: 0.7606\nTraining BiLSTM...\nEpoch: 1\n\tTrain Loss: 0.5535\n\tVal Loss: 0.5264\n\tVal Accuracy: 0.7394\nEpoch: 2\n\tTrain Loss: 0.4614\n\tVal Loss: 0.5067\n\tVal Accuracy: 0.7007\nEpoch: 3\n\tTrain Loss: 0.3525\n\tVal Loss: 0.5019\n\tVal Accuracy: 0.7289\nEpoch: 4\n\tTrain Loss: 0.2659\n\tVal Loss: 0.5877\n\tVal Accuracy: 0.6655\nEpoch: 5\n\tTrain Loss: 0.1554\n\tVal Loss: 0.5894\n\tVal Accuracy: 0.7394\nTraining FacebookAI/xlm-roberta-base...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69cd05a4bcbe4e5aaa6a173550ea12b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f86307ea2e554f2398d6b84770ee4e03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b7199b41a40461c88f7ece910034314"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3d8cf50d9b14a97ba7f5e308b2418b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c44e996737e64d9083890f7f9bd7068b"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250125_200134-lor73yln</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohiuddinprantiq-chittagong-university-of-engineering-an/huggingface/runs/lor73yln' target=\"_blank\">./results_FacebookAI/xlm-roberta-base</a></strong> to <a href='https://wandb.ai/mohiuddinprantiq-chittagong-university-of-engineering-an/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohiuddinprantiq-chittagong-university-of-engineering-an/huggingface' target=\"_blank\">https://wandb.ai/mohiuddinprantiq-chittagong-university-of-engineering-an/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohiuddinprantiq-chittagong-university-of-engineering-an/huggingface/runs/lor73yln' target=\"_blank\">https://wandb.ai/mohiuddinprantiq-chittagong-university-of-engineering-an/huggingface/runs/lor73yln</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='213' max='213' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [213/213 01:22, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.566310</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.499194</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.475989</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Training google-bert/bert-base-multilingual-cased...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a416328f864e4befad5141b702247f20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a75cde193df4fa9bff0f12be710b113"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59724f1ea15844f3b3f9ce70ed34f4b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6e40c65c161424096bd3da2ac4735bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3c04a0150214949bf2e3e13517fb3ca"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='213' max='213' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [213/213 01:08, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.466199</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.458126</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.540794</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nModel Comparison Results for tamil:\n+---------+-------------+----------+------------+\n| Model   |   Precision |   Recall |   F1-Score |\n+=========+=============+==========+============+\n| SVM     |      0.6716 |   0.5056 |     0.5769 |\n+---------+-------------+----------+------------+\n| CNN     |      0.5263 |   0.2247 |     0.315  |\n+---------+-------------+----------+------------+\n| Bi-LSTM |      0.3731 |   0.5618 |     0.4484 |\n+---------+-------------+----------+------------+\n| XLM-R   |      0.9231 |   0.1348 |     0.2353 |\n+---------+-------------+----------+------------+\n| mBERT   |      0.593  |   0.573  |     0.5829 |\n+---------+-------------+----------+------------+\n\nResults saved to 'model_comparison_results_for_tamil.csv'\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# malayalam\ntrain_path = '/kaggle/input/misogyny/misogyny/misogyny/malayalam/train/train.csv'\nval_path = '/kaggle/input/misogyny/misogyny/misogyny/malayalam/dev/dev.csv'\ntest_path = '/kaggle/input/misogyny/test_with_labels_malayalam/test_with_labels.csv'\nresults_mal = run_all_models(train_path, val_path, test_path)\nlan='malayalam'\ndisplay_results(results_mal,lan)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T20:04:20.853862Z","iopub.execute_input":"2025-01-25T20:04:20.854136Z","iopub.status.idle":"2025-01-25T20:07:35.516680Z","shell.execute_reply.started":"2025-01-25T20:04:20.854114Z","shell.execute_reply":"2025-01-25T20:07:35.515833Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nTraining SVM...\nTraining CNN...\nEpoch: 1\n\tTrain Loss: 0.7451\n\tVal Loss: 0.7381\n\tVal Accuracy: 0.5062\nEpoch: 2\n\tTrain Loss: 0.5286\n\tVal Loss: 0.6844\n\tVal Accuracy: 0.5563\nEpoch: 3\n\tTrain Loss: 0.3866\n\tVal Loss: 0.7478\n\tVal Accuracy: 0.5563\nEpoch: 4\n\tTrain Loss: 0.3129\n\tVal Loss: 0.7080\n\tVal Accuracy: 0.5625\nEpoch: 5\n\tTrain Loss: 0.2230\n\tVal Loss: 0.7056\n\tVal Accuracy: 0.5750\nTraining BiLSTM...\nEpoch: 1\n\tTrain Loss: 0.6837\n\tVal Loss: 0.6693\n\tVal Accuracy: 0.6188\nEpoch: 2\n\tTrain Loss: 0.6078\n\tVal Loss: 0.6786\n\tVal Accuracy: 0.5938\nEpoch: 3\n\tTrain Loss: 0.4588\n\tVal Loss: 0.6886\n\tVal Accuracy: 0.6062\nEpoch: 4\n\tTrain Loss: 0.3011\n\tVal Loss: 0.7898\n\tVal Accuracy: 0.6562\nEpoch: 5\n\tTrain Loss: 0.1746\n\tVal Loss: 0.9380\n\tVal Accuracy: 0.6188\nTraining FacebookAI/xlm-roberta-base...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [120/120 00:59, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.579178</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.566238</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.577757</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Training google-bert/bert-base-multilingual-cased...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [120/120 00:46, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.579189</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.564610</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.614726</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nModel Comparison Results for malayalam:\n+---------+-------------+----------+------------+\n| Model   |   Precision |   Recall |   F1-Score |\n+=========+=============+==========+============+\n| SVM     |      0.6538 |   0.6538 |     0.6538 |\n+---------+-------------+----------+------------+\n| CNN     |      0.5132 |   0.5    |     0.5065 |\n+---------+-------------+----------+------------+\n| Bi-LSTM |      0.5046 |   0.7051 |     0.5882 |\n+---------+-------------+----------+------------+\n| XLM-R   |      0.6633 |   0.8333 |     0.7386 |\n+---------+-------------+----------+------------+\n| mBERT   |      0.6981 |   0.4744 |     0.5649 |\n+---------+-------------+----------+------------+\n\nResults saved to 'model_comparison_results_for_malayalam.csv'\n","output_type":"stream"}],"execution_count":14}]}
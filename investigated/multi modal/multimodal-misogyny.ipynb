{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10523956,"sourceType":"datasetVersion","datasetId":6404969}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import precision_recall_fscore_support, f1_score\nfrom torchvision import transforms\nfrom torchvision.transforms import Compose, Resize, ToTensor, Normalize, RandomHorizontalFlip, RandomRotation\nfrom torchvision.models import resnet50, vgg16\nfrom PIL import Image\nfrom tabulate import tabulate\nimport warnings\nfrom transformers import (\n    AutoFeatureExtractor, \n    SwinModel, \n    BertTokenizer, \n    BertModel, \n    XLMRobertaTokenizer, \n    XLMRobertaModel\n)\nwarnings.filterwarnings('ignore')\nprint('imported')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T14:58:58.102333Z","iopub.execute_input":"2025-01-27T14:58:58.102643Z","iopub.status.idle":"2025-01-27T14:58:58.108687Z","shell.execute_reply.started":"2025-01-27T14:58:58.102620Z","shell.execute_reply":"2025-01-27T14:58:58.107901Z"}},"outputs":[{"name":"stdout","text":"imported\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Malayalam\ntrain_csv_mal = '/kaggle/input/misogyny/misogyny/misogyny/malayalam/train/train.csv'\nval_csv_mal = '/kaggle/input/misogyny/misogyny/misogyny/malayalam/dev/dev.csv'\ndf_train_mal = pd.read_csv(train_csv_mal)\ndf_val_mal = pd.read_csv(val_csv_mal)\n    \ntrain_img_dir_mal = '/kaggle/input/misogyny/misogyny/misogyny/malayalam/train/memes'\nval_img_dir_mal = '/kaggle/input/misogyny/misogyny/misogyny/malayalam/dev/memes'\n\ntest_csv_mal = '/kaggle/input/misogyny/test_with_labels_malayalam/test_with_labels.csv'\ntest_img_dir_mal = '/kaggle/input/misogyny/test_with_labels_malayalam/memes'\ndf_test_mal = pd.read_csv(test_csv_mal)\n\n# Tamil\ntrain_csv_tam = '/kaggle/input/misogyny/misogyny/misogyny/tamil/train/train.csv'\nval_csv_tam = '/kaggle/input/misogyny/misogyny/misogyny/tamil/dev/dev.csv'\ndf_train_tam = pd.read_csv(train_csv_tam)\ndf_val_tam = pd.read_csv(val_csv_tam)\n    \ntrain_img_dir_tam = '/kaggle/input/misogyny/misogyny/misogyny/tamil/train/memes'\nval_img_dir_tam = '/kaggle/input/misogyny/misogyny/misogyny/tamil/dev/memes'\n\ntest_csv_tam = '/kaggle/input/misogyny/test_with_labels_tamil/test_with_labels.csv'\ntest_img_dir_tam = '/kaggle/input/misogyny/test_with_labels_tamil/memes'\ndf_test_tam = pd.read_csv(test_csv_tam)\n\nrow_tamil = []\nrow_malayalam = []\nheaders = ['Text Model', 'Image Model', 'Language', 'Precision', 'Recall', 'F1-Score']\nprint('data found')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T14:59:04.001141Z","iopub.execute_input":"2025-01-27T14:59:04.001479Z","iopub.status.idle":"2025-01-27T14:59:04.046019Z","shell.execute_reply.started":"2025-01-27T14:59:04.001449Z","shell.execute_reply":"2025-01-27T14:59:04.045209Z"}},"outputs":[{"name":"stdout","text":"data found\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# svm + resnet50 -> tamil\nclass MemeDataset(Dataset):\n    def __init__(self, data, image_dir, transform=None):\n        self.data = data\n        self.image_dir = image_dir\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        img_name = f\"{self.image_dir}/{self.data.iloc[idx]['image_id']}.jpg\"\n        image = Image.open(img_name).convert('RGB')\n        image = self.transform(image)\n        \n        text = self.data.iloc[idx]['transcriptions']\n        label = self.data.iloc[idx]['labels']\n        \n        return image, text, label\n\ndef extract_text_features(train_data, test_data):\n    vectorizer = TfidfVectorizer(max_features=10000)\n    X_train_text = vectorizer.fit_transform(train_data['transcriptions'])\n    X_test_text = vectorizer.transform(test_data['transcriptions'])\n    \n    svm = LinearSVC(random_state=42)\n    svm.fit(X_train_text, train_data['labels'])\n    \n    train_text_features = svm.decision_function(X_train_text)\n    test_text_features = svm.decision_function(X_test_text)\n    \n    # Convert sparse matrices to dense NumPy arrays\n    return train_text_features.reshape(-1, 1), test_text_features.reshape(-1, 1)\n\ndef extract_image_features(loader):\n    resnet = torchvision.models.resnet50(pretrained=True)\n    resnet = nn.Sequential(*list(resnet.children())[:-1])\n    resnet.eval()\n    \n    all_features, all_labels = [], []\n    \n    with torch.no_grad():\n        for images, _, labels in loader:\n            features = resnet(images).view(images.size(0), -1).cpu().numpy()\n            all_features.append(features)\n            all_labels.extend(labels.tolist())\n    \n    return np.vstack(all_features), np.array(all_labels)\n\nclass MultimodalClassifier(nn.Module):\n    def __init__(self, text_feature_dim, image_feature_dim):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(text_feature_dim + image_feature_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(64, 2)\n        )\n    \n    def forward(self, text_features, image_features):\n        combined = torch.cat((text_features, image_features), dim=1)\n        return self.fc(combined)\n\ndef train_multimodal_model(model, criterion, optimizer, text_features, image_features, labels):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    # Convert data to tensors and move to device\n    text_features = torch.FloatTensor(text_features).to(device)\n    image_features = torch.FloatTensor(image_features).to(device)\n    labels = torch.LongTensor(labels).to(device)\n    \n    model.train()\n    for epoch in range(5):\n        optimizer.zero_grad()\n        outputs = model(text_features, image_features)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        # Calculate training F1 score\n        _, predictions = torch.max(outputs, 1)\n        f1 = f1_score(labels.cpu().numpy(), predictions.cpu().numpy(), average='binary')\n        print(f\"Epoch [{epoch+1}/5], Loss: {loss.item():.4f}, F1 Score: {f1:.4f}\")\n\ndef evaluate_model(model, text_features, image_features, labels):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    # Convert data to tensors and move to device\n    text_features = torch.FloatTensor(text_features).to(device)\n    image_features = torch.FloatTensor(image_features).to(device)\n    labels = torch.LongTensor(labels).to(device)\n    \n    model.eval()\n    with torch.no_grad():\n        outputs = model(text_features, image_features)\n        _, predictions = torch.max(outputs, 1)\n    \n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels.cpu().numpy(), predictions.cpu().numpy(), average='binary'\n    )\n    \n    return precision, recall, f1\n\ndef SVM_ResNet50():\n    # Load data\n    train_data = pd.read_csv(train_csv_tam)\n    test_data = pd.read_csv(test_csv_tam)\n    \n    # Prepare datasets and loaders\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    train_dataset = MemeDataset(train_data, train_img_dir_tam, transform)\n    test_dataset = MemeDataset(test_data, test_img_dir_tam, transform)\n    \n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=16)\n    \n    # Extract features\n    train_text_features, test_text_features = extract_text_features(train_data, test_data)\n    train_image_features, train_labels = extract_image_features(train_loader)\n    test_image_features, test_labels = extract_image_features(test_loader)\n\n    # Normalize features\n    scaler = StandardScaler()\n    train_text_features = scaler.fit_transform(train_text_features)\n    test_text_features = scaler.transform(test_text_features)\n    train_image_features = scaler.fit_transform(train_image_features)\n    test_image_features = scaler.transform(test_image_features)\n    \n    # Train multimodal model\n    model = MultimodalClassifier(\n        text_feature_dim=train_text_features.shape[1],\n        image_feature_dim=train_image_features.shape[1]\n    )\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n    train_multimodal_model(model, criterion, optimizer, train_text_features, train_image_features, train_labels)\n    \n    # Evaluate model\n    precision, recall, f1 = evaluate_model(model, test_text_features, test_image_features, test_labels)\n    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n    row_tamil.append(['SVM','ResNet50','tamil',f'{precision:.4f}',f'{recall:.4f}',f'{f1:.4f}'])\n\nSVM_ResNet50()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T14:59:07.296296Z","iopub.execute_input":"2025-01-27T14:59:07.296623Z","iopub.status.idle":"2025-01-27T15:01:54.800727Z","shell.execute_reply.started":"2025-01-27T14:59:07.296600Z","shell.execute_reply":"2025-01-27T15:01:54.799844Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 192MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/5], Loss: 0.8307, F1 Score: 0.3808\nEpoch [2/5], Loss: 0.8045, F1 Score: 0.4081\nEpoch [3/5], Loss: 0.7754, F1 Score: 0.4260\nEpoch [4/5], Loss: 0.7581, F1 Score: 0.4295\nEpoch [5/5], Loss: 0.7392, F1 Score: 0.4456\nPrecision: 0.3000, Recall: 0.9438, F1 Score: 0.4553\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# svm + swin -> tamil\nclass MemeDataset(Dataset):\n    def __init__(self, data, image_dir, transform=None, is_training=False):\n        self.data = data\n        self.image_dir = image_dir\n        self.is_training = is_training\n        \n        # Define separate transforms for training and evaluation\n        if transform is None:\n            if is_training:\n                self.transform = Compose([\n                    Resize((384, 384)),\n                    RandomHorizontalFlip(p=0.3),\n                    RandomRotation(15),\n                    ToTensor(),\n                    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n                ])\n            else:\n                self.transform = Compose([\n                    Resize((384, 384)),\n                    ToTensor(),\n                    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n                ])\n        else:\n            self.transform = transform\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        img_name = f\"{self.image_dir}/{self.data.iloc[idx]['image_id']}.jpg\"\n        image = Image.open(img_name).convert('RGB')\n        image = self.transform(image)\n        \n        text = str(self.data.iloc[idx]['transcriptions'])\n        label = self.data.iloc[idx]['labels']\n        \n        return {\n            'image': image,\n            'text': text,\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\nclass MultimodalDataset:\n    def __init__(self, text_features, image_features, labels):\n        self.text_features = torch.FloatTensor(text_features)\n        self.image_features = torch.FloatTensor(image_features)\n        self.labels = torch.LongTensor(labels)\n        \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return {\n            'text': self.text_features[idx],\n            'image': self.image_features[idx],\n            'label': self.labels[idx]\n        }\n\ndef extract_text_features(train_data, val_data, test_data):\n    # Create TF-IDF vectorizer with better parameters\n    vectorizer = TfidfVectorizer(\n        max_features=10000,  # Increased from 5000\n        ngram_range=(1, 2),  # Added bigrams\n        min_df=2,  # Remove very rare terms\n        max_df=0.95,  # Remove very common terms\n        strip_accents='unicode',\n        lowercase=True\n    )\n    \n    # Fit on training data only\n    X_train_text = vectorizer.fit_transform(train_data['transcriptions'])\n    X_val_text = vectorizer.transform(val_data['transcriptions'])\n    X_test_text = vectorizer.transform(test_data['transcriptions'])\n    \n    # Train SVM with better parameters\n    svm = LinearSVC(\n        C=1.0,\n        class_weight='balanced',\n        dual=False,\n        max_iter=2000,\n        random_state=42\n    )\n    svm.fit(X_train_text, train_data['labels'])\n    \n    # Get decision function scores\n    train_text_features = svm.decision_function(X_train_text)\n    val_text_features = svm.decision_function(X_val_text)\n    test_text_features = svm.decision_function(X_test_text)\n    \n    return (train_text_features.reshape(-1, 1), \n            val_text_features.reshape(-1, 1), \n            test_text_features.reshape(-1, 1))\n\ndef extract_image_features(loader, swin_model, feature_extractor, device):\n    features_list, labels_list = [], []\n    \n    swin_model.eval()\n    with torch.no_grad():\n        for batch in loader:\n            images = batch['image'].to(device)\n            labels = batch['label']\n            \n            # Get Swin features\n            outputs = swin_model(images)\n            \n            # Use pooled output instead of mean of last hidden state\n            features = outputs.pooler_output.cpu().numpy()\n            \n            features_list.append(features)\n            labels_list.extend(labels.numpy())\n    \n    return np.vstack(features_list), np.array(labels_list)\n\nclass MultimodalClassifier(nn.Module):\n    def __init__(self, text_feature_dim, image_feature_dim):\n        super().__init__()\n        \n        # Separate feature processing\n        self.text_processor = nn.Sequential(\n            nn.Linear(text_feature_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        \n        self.image_processor = nn.Sequential(\n            nn.Linear(image_feature_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        \n        # Fusion layers\n        self.fusion = nn.Sequential(\n            nn.Linear(256 + 512, 384),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(384, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 2)\n        )\n        \n        # Batch Normalization layers\n        self.bn_text = nn.BatchNorm1d(256)\n        self.bn_image = nn.BatchNorm1d(512)\n        self.bn_fusion = nn.BatchNorm1d(384)\n    \n    def forward(self, text_features, image_features):\n        # Process text features\n        text_features = self.text_processor(text_features)\n        text_features = self.bn_text(text_features)\n        \n        # Process image features\n        image_features = self.image_processor(image_features)\n        image_features = self.bn_image(image_features)\n        \n        # Combine features\n        combined = torch.cat((text_features, image_features), dim=1)\n        \n        # Final classification\n        output = self.fusion(combined)\n        return output\n\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    for batch in dataloader:\n        text_features = batch['text'].to(device)\n        image_features = batch['image'].to(device)\n        labels = batch['label'].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(text_features, image_features)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        _, preds = torch.max(outputs, 1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n    \n    metrics = precision_recall_fscore_support(\n        all_labels, all_preds, average='binary', zero_division=0\n    )\n    return total_loss / len(dataloader), metrics[2]  # Return loss and F1\n\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    total_loss = 0\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            text_features = batch['text'].to(device)\n            image_features = batch['image'].to(device)\n            labels = batch['label'].to(device)\n            \n            outputs = model(text_features, image_features)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            \n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    precision, recall, f1, _ = precision_recall_fscore_support(\n        all_labels, all_preds, average='binary', zero_division=0\n    )\n    return total_loss / len(dataloader), precision, recall, f1\n\ndef SVM_SwinTransformer(train_data, val_data, test_data, train_img_dir, val_img_dir, test_img_dir):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Create datasets with augmentation for training\n    train_dataset = MemeDataset(train_data, train_img_dir, is_training=True)\n    val_dataset = MemeDataset(val_data, val_img_dir, is_training=False)\n    test_dataset = MemeDataset(test_data, test_img_dir, is_training=False)\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=16)\n    test_loader = DataLoader(test_dataset, batch_size=16)\n    \n    # Initialize Swin Transformer\n    swin_model = SwinModel.from_pretrained(\"microsoft/swin-base-patch4-window12-384\").to(device)\n    feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-base-patch4-window12-384\")\n    \n    # Extract text features\n    train_text_features, val_text_features, test_text_features = extract_text_features(\n        train_data, val_data, test_data\n    )\n    \n    # Extract image features\n    train_image_features, train_labels = extract_image_features(train_loader, swin_model, feature_extractor, device)\n    val_image_features, val_labels = extract_image_features(val_loader, swin_model, feature_extractor, device)\n    test_image_features, test_labels = extract_image_features(test_loader, swin_model, feature_extractor, device)\n    \n    # Normalize features\n    scaler = StandardScaler()\n    train_text_features = scaler.fit_transform(train_text_features)\n    val_text_features = scaler.transform(val_text_features)\n    test_text_features = scaler.transform(test_text_features)\n    \n    scaler_img = StandardScaler()\n    train_image_features = scaler_img.fit_transform(train_image_features)\n    val_image_features = scaler_img.transform(val_image_features)\n    test_image_features = scaler_img.transform(test_image_features)\n\n    # Create combined datasets with processed features\n    train_combined_dataset = MultimodalDataset(train_text_features, train_image_features, train_labels)\n    val_combined_dataset = MultimodalDataset(val_text_features, val_image_features, val_labels)\n    test_combined_dataset = MultimodalDataset(test_text_features, test_image_features, test_labels)\n\n    # Create dataloaders for training\n    train_loader = DataLoader(train_combined_dataset, batch_size=16, shuffle=True)\n    val_loader = DataLoader(val_combined_dataset, batch_size=16)\n    test_loader = DataLoader(test_combined_dataset, batch_size=16)\n    \n    # Initialize model\n    model = MultimodalClassifier(\n        text_feature_dim=train_text_features.shape[1],\n        image_feature_dim=train_image_features.shape[1]\n    ).to(device)\n    \n    # Training settings\n    criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0]).to(device))  # Weight minority class more\n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n    \n    # Training loop\n    best_val_f1 = 0\n    epochs = 5\n    for epoch in range(epochs):\n        # Train\n        train_loss, train_f1 = train_epoch(model, train_loader, criterion, optimizer, device)\n        \n        # Validate\n        val_loss, val_prec, val_recall, val_f1 = evaluate(model, val_loader, criterion, device)\n        \n        # Learning rate scheduling\n        scheduler.step(val_f1)\n        \n        print(f\"Epoch {epoch+1}/{epochs}\")\n        print(f\"Train - Loss: {train_loss:.4f}, F1: {train_f1:.4f}\")\n        print(f\"Val - Loss: {val_loss:.4f}, F1: {val_f1:.4f}\")\n        \n        # Save best model\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            torch.save(model.state_dict(), 'best_model.pt')\n    \n    # Load best model and evaluate\n    model.load_state_dict(torch.load('best_model.pt'))\n    test_loss, test_prec, test_recall, test_f1 = evaluate(model, test_loader, criterion, device)\n    print(f\"\\nTest Results:\")\n    print(f\"Precision: {test_prec:.4f}\")\n    print(f\"Recall: {test_recall:.4f}\")\n    print(f\"F1 Score: {test_f1:.4f}\")\n    row_tamil.append(['SVM','Swin','tamil',f'{test_prec:.4f}',f'{test_recall:.4f}',f'{test_f1:.4f}'])\n\n# Run the model\nSVM_SwinTransformer(df_train_tam, df_val_tam, df_test_tam, train_img_dir_tam, val_img_dir_tam, test_img_dir_tam)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T15:02:15.456965Z","iopub.execute_input":"2025-01-27T15:02:15.457239Z","iopub.status.idle":"2025-01-27T15:03:31.789285Z","shell.execute_reply.started":"2025-01-27T15:02:15.457219Z","shell.execute_reply":"2025-01-27T15:03:31.788435Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/71.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ec7298c99ac417f9c8000d006266883"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/356M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8d5a8dc0572482f9a190c8de045f0aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba77957fbee0415386009dadff43e88e"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/5\nTrain - Loss: 0.7089, F1: 0.3799\nVal - Loss: 0.6794, F1: 0.4831\nEpoch 2/5\nTrain - Loss: 0.6704, F1: 0.4197\nVal - Loss: 0.6583, F1: 0.4675\nEpoch 3/5\nTrain - Loss: 0.6373, F1: 0.4770\nVal - Loss: 0.6398, F1: 0.4615\nEpoch 4/5\nTrain - Loss: 0.6120, F1: 0.5250\nVal - Loss: 0.6215, F1: 0.4559\nEpoch 5/5\nTrain - Loss: 0.5918, F1: 0.5634\nVal - Loss: 0.6169, F1: 0.4706\n\nTest Results:\nPrecision: 0.3020\nRecall: 0.6854\nF1 Score: 0.4192\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# mBERT + resnet50 -> tamil\nclass MemeDataset(Dataset):\n    def __init__(self, dataframe, image_dir, tokenizer, max_length, transform=None):\n        self.data = dataframe\n        self.image_dir = image_dir\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.transform = transform if transform else Compose([\n            Resize((224, 224)),\n            ToTensor(),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n        ])\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        img_path = f\"{self.image_dir}/{row['image_id']}.jpg\"\n        image = Image.open(img_path).convert('RGB')\n        image = self.transform(image)\n        \n        encoding = self.tokenizer(\n            str(row['transcriptions']),  # Convert to string to handle potential non-string inputs\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        return {\n            'image': image,\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'label': torch.tensor(row['labels'], dtype=torch.long)\n        }\n\nclass MultimodalClassifier(nn.Module):\n    def __init__(self, text_model, image_model, num_classes=2):\n        super().__init__()\n        self.text_model = text_model\n        self.image_model = image_model\n        \n        # Get feature dimensions\n        self.text_feature_dim = text_model.config.hidden_size  # Usually 768 for BERT\n        self.image_feature_dim = 2048  # ResNet50 feature dimension\n        \n        # Fusion layers\n        self.fusion = nn.Sequential(\n            nn.Linear(self.text_feature_dim + self.image_feature_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, input_ids, attention_mask, images):\n        # Text features\n        text_outputs = self.text_model(input_ids, attention_mask=attention_mask)\n        text_features = text_outputs.last_hidden_state[:, 0, :]  # Use [CLS] token\n        \n        # Image features\n        image_features = self.image_model(images)\n        \n        # Concatenate and classify\n        combined = torch.cat((text_features, image_features), dim=1)\n        return self.fusion(combined)\n\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    for batch in dataloader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        images = batch['image'].to(device)\n        labels = batch['label'].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask, images)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        _, preds = torch.max(outputs, 1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n    \n    # Calculate metrics\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        all_labels, all_preds, average='binary', zero_division=0\n    )\n    \n    return total_loss / len(dataloader), precision, recall, f1\n\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            images = batch['image'].to(device)\n            labels = batch['label'].to(device)\n            \n            outputs = model(input_ids, attention_mask, images)\n            loss = criterion(outputs, labels)\n            \n            total_loss += loss.item()\n            \n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Calculate metrics\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        all_labels, all_preds, average='binary', zero_division=0\n    )\n    \n    return total_loss / len(dataloader), precision, recall, f1\n\ndef mBERT_ResNet50(df_train, df_val, df_test, train_img_dir, val_img_dir, test_img_dir):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Initialize models\n    tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n    text_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n    \n    image_model = resnet50(pretrained=True)\n    image_model.fc = nn.Identity()  # Remove classification head\n    \n    # Create datasets and dataloaders\n    train_dataset = MemeDataset(df_train, train_img_dir, tokenizer, max_length=128)\n    val_dataset = MemeDataset(df_val, val_img_dir, tokenizer, max_length=128)\n    test_dataset = MemeDataset(df_test, test_img_dir, tokenizer, max_length=128)\n    \n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=16)\n    test_loader = DataLoader(test_dataset, batch_size=16)\n    \n    # Initialize multimodal model\n    model = MultimodalClassifier(text_model, image_model)\n    model = model.to(device)\n    \n    # Training settings\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n    num_epochs = 5\n    \n    # Training loop\n    best_val_f1 = 0\n    for epoch in range(num_epochs):\n        # Train\n        train_loss, train_prec, train_recall, train_f1 = train_epoch(\n            model, train_loader, criterion, optimizer, device\n        )\n        \n        # Validate\n        val_loss, val_prec, val_recall, val_f1 = evaluate(\n            model, val_loader, criterion, device\n        )\n        \n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"Train - Loss: {train_loss:.4f}, F1: {train_f1:.4f}\")\n        print(f\"Val - Loss: {val_loss:.4f}, F1: {val_f1:.4f}\")\n        \n        # Save best model\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            torch.save(model.state_dict(), 'best_model.pt')\n    \n    # Load best model and evaluate on test set\n    model.load_state_dict(torch.load('best_model.pt'))\n    test_loss, test_prec, test_recall, test_f1 = evaluate(\n        model, test_loader, criterion, device\n    )\n    print(f\"\\nTest Results:\")\n    print(f\"Precision: {test_prec:.4f}\")\n    print(f\"Recall: {test_recall:.4f}\")\n    print(f\"F1 Score: {test_f1:.4f}\")\n    row_tamil.append(['mBERT','ResNet50','tamil',f'{test_prec:.4f}',f'{test_recall:.4f}',f'{test_f1:.4f}'])\n\n# Run the model\nmBERT_ResNet50(df_train_tam, df_val_tam, df_test_tam, train_img_dir_tam, val_img_dir_tam, test_img_dir_tam)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T15:04:05.673070Z","iopub.execute_input":"2025-01-27T15:04:05.673407Z","iopub.status.idle":"2025-01-27T15:07:58.032326Z","shell.execute_reply.started":"2025-01-27T15:04:05.673375Z","shell.execute_reply":"2025-01-27T15:07:58.031412Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75338d402bb3455ab64d5bfdcf7b224f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bfd34ae32534f859c7f48de2736bcf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73370d49f79b496db51d9b2bafe80ddd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d402b60162034d1f92d7500961869d4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0534f9930b8d419aa1289da045b481ec"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/5\nTrain - Loss: 0.5348, F1: 0.0138\nVal - Loss: 0.4775, F1: 0.0000\nEpoch 2/5\nTrain - Loss: 0.3697, F1: 0.5169\nVal - Loss: 0.4105, F1: 0.5000\nEpoch 3/5\nTrain - Loss: 0.1747, F1: 0.9049\nVal - Loss: 0.5002, F1: 0.5484\nEpoch 4/5\nTrain - Loss: 0.0678, F1: 0.9843\nVal - Loss: 0.5311, F1: 0.5512\nEpoch 5/5\nTrain - Loss: 0.0346, F1: 0.9877\nVal - Loss: 0.5590, F1: 0.6184\n\nTest Results:\nPrecision: 0.6596\nRecall: 0.6966\nF1 Score: 0.6776\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# mBERT + swin -> tamil\nclass MemeDataset(Dataset):\n    def __init__(self, dataframe, image_dir, tokenizer, max_length, transform=None):\n        self.data = dataframe\n        self.image_dir = image_dir\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.transform = transform if transform else Compose([\n            Resize((224, 224)),\n            ToTensor(),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        img_path = f\"{self.image_dir}/{row['image_id']}.jpg\"\n        image = Image.open(img_path).convert('RGB')\n        image = self.transform(image)\n        \n        encoding = self.tokenizer(\n            str(row['transcriptions']),\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        return {\n            'image': image,\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'label': torch.tensor(row['labels'], dtype=torch.long)\n        }\n\nclass MultimodalClassifier(nn.Module):\n    def __init__(self, text_model, image_model, num_classes=2):\n        super().__init__()\n        self.text_model = text_model\n        self.image_model = image_model\n        \n        # Get feature dimensions\n        self.text_feature_dim = text_model.config.hidden_size  # 768 for BERT\n        self.image_feature_dim = image_model.config.hidden_size  # 768 for Swin-Base\n        \n        # Fusion layers\n        self.fusion = nn.Sequential(\n            nn.Linear(self.text_feature_dim + self.image_feature_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, input_ids, attention_mask, images):\n        # Text features\n        text_outputs = self.text_model(input_ids, attention_mask=attention_mask)\n        text_features = text_outputs.last_hidden_state[:, 0, :]  # Use [CLS] token\n        \n        # Image features\n        image_outputs = self.image_model(images)\n        image_features = image_outputs.pooler_output  # Use pooled output from Swin\n        \n        # Concatenate and classify\n        combined = torch.cat((text_features, image_features), dim=1)\n        return self.fusion(combined)\n\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    for batch in dataloader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        images = batch['image'].to(device)\n        labels = batch['label'].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask, images)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        _, preds = torch.max(outputs, 1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n    \n    # Calculate metrics\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        all_labels, all_preds, average='binary', zero_division=0\n    )\n    \n    return total_loss / len(dataloader), precision, recall, f1\n\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            images = batch['image'].to(device)\n            labels = batch['label'].to(device)\n            \n            outputs = model(input_ids, attention_mask, images)\n            loss = criterion(outputs, labels)\n            \n            total_loss += loss.item()\n            \n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Calculate metrics\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        all_labels, all_preds, average='binary', zero_division=0\n    )\n    \n    return total_loss / len(dataloader), precision, recall, f1\n\ndef mBERT_SwinTransformer(df_train, df_val, df_test, train_img_dir, val_img_dir, test_img_dir):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Initialize models\n    tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n    text_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n    \n    # Initialize Swin Transformer\n    image_model = SwinModel.from_pretrained(\"microsoft/swin-base-patch4-window7-224-in22k\")\n    \n    # Create datasets and dataloaders\n    train_dataset = MemeDataset(df_train, train_img_dir, tokenizer, max_length=128)\n    val_dataset = MemeDataset(df_val, val_img_dir, tokenizer, max_length=128)\n    test_dataset = MemeDataset(df_test, test_img_dir, tokenizer, max_length=128)\n    \n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=16)\n    test_loader = DataLoader(test_dataset, batch_size=16)\n    \n    # Initialize multimodal model\n    model = MultimodalClassifier(text_model, image_model)\n    model = model.to(device)\n    \n    # Training settings\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n    num_epochs = 5\n    \n    # Training loop\n    best_val_f1 = 0\n    for epoch in range(num_epochs):\n        # Train\n        train_loss, train_prec, train_recall, train_f1 = train_epoch(\n            model, train_loader, criterion, optimizer, device\n        )\n        \n        # Validate\n        val_loss, val_prec, val_recall, val_f1 = evaluate(\n            model, val_loader, criterion, device\n        )\n        \n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"Train - Loss: {train_loss:.4f}, F1: {train_f1:.4f}\")\n        print(f\"Val - Loss: {val_loss:.4f}, F1: {val_f1:.4f}\")\n        \n        # Save best model\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            torch.save(model.state_dict(), 'best_model.pt')\n    \n    # Load best model and evaluate on test set\n    model.load_state_dict(torch.load('best_model.pt'))\n    test_loss, test_prec, test_recall, test_f1 = evaluate(\n        model, test_loader, criterion, device\n    )\n    print(f\"\\nTest Results:\")\n    print(f\"Precision: {test_prec:.4f}\")\n    print(f\"Recall: {test_recall:.4f}\")\n    print(f\"F1 Score: {test_f1:.4f}\")\n    row_tamil.append(['mBERT','Swin','tamil',f'{test_prec:.4f}',f'{test_recall:.4f}',f'{test_f1:.4f}'])\n\n# Run the model\nmBERT_SwinTransformer(df_train_tam, df_val_tam, df_test_tam, train_img_dir_tam, val_img_dir_tam, test_img_dir_tam)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T15:08:09.407878Z","iopub.execute_input":"2025-01-27T15:08:09.408254Z","iopub.status.idle":"2025-01-27T15:13:33.815357Z","shell.execute_reply.started":"2025-01-27T15:08:09.408207Z","shell.execute_reply":"2025-01-27T15:13:33.814468Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1eceb33a4fb3418795b0b558e5747a53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/437M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c20bd14103224d07b5bdae0b25660ffd"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/5\nTrain - Loss: 0.5049, F1: 0.1039\nVal - Loss: 0.4569, F1: 0.5426\nEpoch 2/5\nTrain - Loss: 0.3562, F1: 0.6160\nVal - Loss: 0.4611, F1: 0.4952\nEpoch 3/5\nTrain - Loss: 0.2282, F1: 0.8216\nVal - Loss: 0.4142, F1: 0.6259\nEpoch 4/5\nTrain - Loss: 0.1326, F1: 0.9184\nVal - Loss: 0.4825, F1: 0.6522\nEpoch 5/5\nTrain - Loss: 0.0644, F1: 0.9665\nVal - Loss: 0.5691, F1: 0.6560\n\nTest Results:\nPrecision: 0.7463\nRecall: 0.5618\nF1 Score: 0.6410\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(f\"\\nComparison Results for Tamil:\")\nprint(tabulate(row_tamil, headers=headers, tablefmt='grid'))\n\nresults_tamil = pd.DataFrame(row_tamil, columns=headers)\nresults_tamil.to_csv(f'Comparison_Results_Tamil.csv', index=False)\nprint(f\"\\nResults saved to 'Comparison_Results_for_Tamil.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T15:13:38.847744Z","iopub.execute_input":"2025-01-27T15:13:38.848036Z","iopub.status.idle":"2025-01-27T15:13:38.862083Z","shell.execute_reply.started":"2025-01-27T15:13:38.848014Z","shell.execute_reply":"2025-01-27T15:13:38.861398Z"}},"outputs":[{"name":"stdout","text":"\nComparison Results for Tamil:\n+--------------+---------------+------------+-------------+----------+------------+\n| Text Model   | Image Model   | Language   |   Precision |   Recall |   F1-Score |\n+==============+===============+============+=============+==========+============+\n| SVM          | ResNet50      | tamil      |      0.3    |   0.9438 |     0.4553 |\n+--------------+---------------+------------+-------------+----------+------------+\n| SVM          | Swin          | tamil      |      0.302  |   0.6854 |     0.4192 |\n+--------------+---------------+------------+-------------+----------+------------+\n| mBERT        | ResNet50      | tamil      |      0.6596 |   0.6966 |     0.6776 |\n+--------------+---------------+------------+-------------+----------+------------+\n| mBERT        | Swin          | tamil      |      0.7463 |   0.5618 |     0.641  |\n+--------------+---------------+------------+-------------+----------+------------+\n\nResults saved to 'Comparison_Results_for_Tamil.csv'\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# xlmr + vgg16 -> mal\nclass MemeDataset(Dataset):\n    def __init__(self, dataframe, image_dir, tokenizer, max_length, transform=None):\n        self.data = dataframe\n        self.image_dir = image_dir\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.transform = transform if transform else Compose([\n            Resize((224, 224)),\n            ToTensor(),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        img_path = f\"{self.image_dir}/{row['image_id']}.jpg\"\n        image = Image.open(img_path).convert('RGB')\n        image = self.transform(image)\n        \n        encoding = self.tokenizer(\n            str(row['transcriptions']),\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        return {\n            'image': image,\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'label': torch.tensor(row['labels'], dtype=torch.long)\n        }\n\nclass MultimodalClassifier(nn.Module):\n    def __init__(self, text_model, image_model, num_classes=2):\n        super().__init__()\n        self.text_model = text_model\n        self.image_model = image_model\n        \n        # Get feature dimensions\n        self.text_feature_dim = text_model.config.hidden_size  # 768 for XLM-R base, 1024 for large\n        self.image_feature_dim = 4096  # VGG16 feature dimension after last FC layer\n        \n        # Fusion layers\n        self.fusion = nn.Sequential(\n            nn.Linear(self.text_feature_dim + self.image_feature_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, input_ids, attention_mask, images):\n        # Text features\n        text_outputs = self.text_model(input_ids, attention_mask=attention_mask)\n        text_features = text_outputs.last_hidden_state[:, 0, :]  # Use [CLS] token\n        \n        # Image features\n        image_features = self.image_model(images)\n        \n        # Concatenate and classify\n        combined = torch.cat((text_features, image_features), dim=1)\n        return self.fusion(combined)\n\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    for batch in dataloader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        images = batch['image'].to(device)\n        labels = batch['label'].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask, images)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        _, preds = torch.max(outputs, 1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n    \n    precision, recall, f1, _ = precision_recall_fscore_support(\n        all_labels, all_preds, average='binary', zero_division=0\n    )\n    \n    return total_loss / len(dataloader), precision, recall, f1\n\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            images = batch['image'].to(device)\n            labels = batch['label'].to(device)\n            \n            outputs = model(input_ids, attention_mask, images)\n            loss = criterion(outputs, labels)\n            \n            total_loss += loss.item()\n            \n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    precision, recall, f1, _ = precision_recall_fscore_support(\n        all_labels, all_preds, average='binary', zero_division=0\n    )\n    \n    return total_loss / len(dataloader), precision, recall, f1\n\ndef XLMR_VGG16(df_train, df_val, df_test, train_img_dir, val_img_dir, test_img_dir):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Initialize models\n    tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n    text_model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n    \n    # Load VGG16 and modify it\n    image_model = vgg16(pretrained=True)\n    image_model.classifier = nn.Sequential(*list(image_model.classifier.children())[:-1])  # Remove last FC layer\n    \n    # Create datasets and dataloaders\n    train_dataset = MemeDataset(df_train, train_img_dir, tokenizer, max_length=128)\n    val_dataset = MemeDataset(df_val, val_img_dir, tokenizer, max_length=128)\n    test_dataset = MemeDataset(df_test, test_img_dir, tokenizer, max_length=128)\n    \n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=16)\n    test_loader = DataLoader(test_dataset, batch_size=16)\n    \n    # Initialize multimodal model\n    model = MultimodalClassifier(text_model, image_model)\n    model = model.to(device)\n    \n    # Training settings\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n    num_epochs = 5\n    \n    # Training loop\n    best_val_f1 = 0\n    for epoch in range(num_epochs):\n        # Train\n        train_loss, train_prec, train_recall, train_f1 = train_epoch(\n            model, train_loader, criterion, optimizer, device\n        )\n        \n        # Validate\n        val_loss, val_prec, val_recall, val_f1 = evaluate(\n            model, val_loader, criterion, device\n        )\n        \n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"Train - Loss: {train_loss:.4f}, F1: {train_f1:.4f}\")\n        print(f\"Val - Loss: {val_loss:.4f}, F1: {val_f1:.4f}\")\n        \n        # Save best model\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            torch.save(model.state_dict(), 'best_model.pt')\n    \n    # Load best model and evaluate on test set\n    model.load_state_dict(torch.load('best_model.pt'))\n    test_loss, test_prec, test_recall, test_f1 = evaluate(\n        model, test_loader, criterion, device\n    )\n    print(f\"\\nTest Results:\")\n    print(f\"Precision: {test_prec:.4f}\")\n    print(f\"Recall: {test_recall:.4f}\")\n    print(f\"F1 Score: {test_f1:.4f}\")\n    row_malayalam.append(['XLM-R','VGG16','malayalam',f'{test_prec:.4f}',f'{test_recall:.4f}',f'{test_f1:.4f}'])\n\n# Run the model\nXLMR_VGG16(df_train_mal, df_val_mal, df_test_mal, train_img_dir_mal, val_img_dir_mal, test_img_dir_mal)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T15:14:13.617806Z","iopub.execute_input":"2025-01-27T15:14:13.618123Z","iopub.status.idle":"2025-01-27T15:17:42.889809Z","shell.execute_reply.started":"2025-01-27T15:14:13.618096Z","shell.execute_reply":"2025-01-27T15:17:42.888860Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3193c516ca144ec8be78c6f7afe58488"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"142e04361da944a7aef4cb9d1cf18131"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"640420a3783f4f45ab24327deae28997"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d371470dc944416a08e2baa3ce7ef15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0332bd87262a47c8a136c28060ea5325"}},"metadata":{}},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:02<00:00, 223MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5\nTrain - Loss: 0.6717, F1: 0.2443\nVal - Loss: 0.6189, F1: 0.6526\nEpoch 2/5\nTrain - Loss: 0.5635, F1: 0.5564\nVal - Loss: 0.4541, F1: 0.7520\nEpoch 3/5\nTrain - Loss: 0.3755, F1: 0.7683\nVal - Loss: 0.5807, F1: 0.6879\nEpoch 4/5\nTrain - Loss: 0.2545, F1: 0.8630\nVal - Loss: 0.5191, F1: 0.7727\nEpoch 5/5\nTrain - Loss: 0.1322, F1: 0.9486\nVal - Loss: 0.3736, F1: 0.8333\n\nTest Results:\nPrecision: 0.9016\nRecall: 0.7051\nF1 Score: 0.7914\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# xlmr + swin -> malayalam\nclass MemeDataset(Dataset):\n    def __init__(self, dataframe, image_dir, tokenizer, max_length, transform=None):\n        self.data = dataframe\n        self.image_dir = image_dir\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.transform = transform if transform else Compose([\n            Resize((224, 224)),\n            ToTensor(),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        img_path = f\"{self.image_dir}/{row['image_id']}.jpg\"\n        image = Image.open(img_path).convert('RGB')\n        image = self.transform(image)\n        \n        encoding = self.tokenizer(\n            str(row['transcriptions']),\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        return {\n            'image': image,\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'label': torch.tensor(row['labels'], dtype=torch.long)\n        }\n\nclass MultimodalClassifier(nn.Module):\n    def __init__(self, text_model, image_model, num_classes=2):\n        super().__init__()\n        self.text_model = text_model\n        self.image_model = image_model\n        \n        # Get feature dimensions\n        self.text_feature_dim = text_model.config.hidden_size  # 768 for XLM-R base, 1024 for large\n        self.image_feature_dim = image_model.config.hidden_size  # 768 for Swin-Base\n        \n        # Fusion layers\n        self.fusion = nn.Sequential(\n            nn.Linear(self.text_feature_dim + self.image_feature_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, input_ids, attention_mask, images):\n        # Text features\n        text_outputs = self.text_model(input_ids, attention_mask=attention_mask)\n        text_features = text_outputs.last_hidden_state[:, 0, :]  # Use [CLS] token\n        \n        # Image features\n        image_outputs = self.image_model(images)\n        image_features = image_outputs.pooler_output  # Use pooled output from Swin\n        \n        # Concatenate and classify\n        combined = torch.cat((text_features, image_features), dim=1)\n        return self.fusion(combined)\n\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    for batch in dataloader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        images = batch['image'].to(device)\n        labels = batch['label'].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask, images)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        _, preds = torch.max(outputs, 1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n    \n    # Calculate metrics\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        all_labels, all_preds, average='binary', zero_division=0\n    )\n    \n    return total_loss / len(dataloader), precision, recall, f1\n\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            images = batch['image'].to(device)\n            labels = batch['label'].to(device)\n            \n            outputs = model(input_ids, attention_mask, images)\n            loss = criterion(outputs, labels)\n            \n            total_loss += loss.item()\n            \n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Calculate metrics\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        all_labels, all_preds, average='binary', zero_division=0\n    )\n    \n    return total_loss / len(dataloader), precision, recall, f1\n\ndef XLMR_SwinTransformer(df_train, df_val, df_test, train_img_dir, val_img_dir, test_img_dir):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Initialize models\n    tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n    text_model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n    \n    # Initialize Swin Transformer\n    image_model = SwinModel.from_pretrained(\"microsoft/swin-base-patch4-window7-224-in22k\")\n    \n    # Create datasets and dataloaders\n    train_dataset = MemeDataset(df_train, train_img_dir, tokenizer, max_length=128)\n    val_dataset = MemeDataset(df_val, val_img_dir, tokenizer, max_length=128)\n    test_dataset = MemeDataset(df_test, test_img_dir, tokenizer, max_length=128)\n    \n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=16)\n    test_loader = DataLoader(test_dataset, batch_size=16)\n    \n    # Initialize multimodal model\n    model = MultimodalClassifier(text_model, image_model)\n    model = model.to(device)\n    \n    # Training settings\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n    num_epochs = 5\n    \n    # Training loop\n    best_val_f1 = 0\n    for epoch in range(num_epochs):\n        # Train\n        train_loss, train_prec, train_recall, train_f1 = train_epoch(\n            model, train_loader, criterion, optimizer, device\n        )\n        \n        # Validate\n        val_loss, val_prec, val_recall, val_f1 = evaluate(\n            model, val_loader, criterion, device\n        )\n        \n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"Train - Loss: {train_loss:.4f}, F1: {train_f1:.4f}\")\n        print(f\"Val - Loss: {val_loss:.4f}, F1: {val_f1:.4f}\")\n        \n        # Save best model\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            torch.save(model.state_dict(), 'best_model.pt')\n    \n    # Load best model and evaluate on test set\n    model.load_state_dict(torch.load('best_model.pt'))\n    test_loss, test_prec, test_recall, test_f1 = evaluate(\n        model, test_loader, criterion, device\n    )\n    print(f\"\\nTest Results:\")\n    print(f\"Precision: {test_prec:.4f}\")\n    print(f\"Recall: {test_recall:.4f}\")\n    print(f\"F1 Score: {test_f1:.4f}\")\n    row_malayalam.append(['XLM-R','Swin','malayalam',f'{test_prec:.4f}',f'{test_recall:.4f}',f'{test_f1:.4f}'])\n\n# Run the model\nXLMR_SwinTransformer(df_train_mal, df_val_mal, df_test_mal, train_img_dir_mal, val_img_dir_mal, test_img_dir_mal)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T15:17:42.891511Z","iopub.execute_input":"2025-01-27T15:17:42.891770Z","iopub.status.idle":"2025-01-27T15:21:30.470992Z","shell.execute_reply.started":"2025-01-27T15:17:42.891743Z","shell.execute_reply":"2025-01-27T15:21:30.470053Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\nTrain - Loss: 0.6150, F1: 0.6048\nVal - Loss: 0.4542, F1: 0.8000\nEpoch 2/5\nTrain - Loss: 0.3687, F1: 0.8255\nVal - Loss: 0.3186, F1: 0.8033\nEpoch 3/5\nTrain - Loss: 0.2037, F1: 0.9228\nVal - Loss: 0.3367, F1: 0.8136\nEpoch 4/5\nTrain - Loss: 0.1724, F1: 0.9325\nVal - Loss: 0.4361, F1: 0.8358\nEpoch 5/5\nTrain - Loss: 0.0453, F1: 0.9884\nVal - Loss: 0.4610, F1: 0.8254\n\nTest Results:\nPrecision: 0.8312\nRecall: 0.8205\nF1 Score: 0.8258\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# svm + swin -> malayalam\nclass MemeDataset(Dataset):\n    def __init__(self, data, image_dir, transform=None, is_training=False):\n        self.data = data\n        self.image_dir = image_dir\n        self.is_training = is_training\n        \n        # Define separate transforms for training and evaluation\n        if transform is None:\n            if is_training:\n                self.transform = Compose([\n                    Resize((384, 384)),\n                    RandomHorizontalFlip(p=0.3),\n                    RandomRotation(15),\n                    ToTensor(),\n                    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n                ])\n            else:\n                self.transform = Compose([\n                    Resize((384, 384)),\n                    ToTensor(),\n                    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n                ])\n        else:\n            self.transform = transform\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        img_name = f\"{self.image_dir}/{self.data.iloc[idx]['image_id']}.jpg\"\n        image = Image.open(img_name).convert('RGB')\n        image = self.transform(image)\n        \n        text = str(self.data.iloc[idx]['transcriptions'])\n        label = self.data.iloc[idx]['labels']\n        \n        return {\n            'image': image,\n            'text': text,\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\nclass MultimodalDataset:\n    def __init__(self, text_features, image_features, labels):\n        self.text_features = torch.FloatTensor(text_features)\n        self.image_features = torch.FloatTensor(image_features)\n        self.labels = torch.LongTensor(labels)\n        \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return {\n            'text': self.text_features[idx],\n            'image': self.image_features[idx],\n            'label': self.labels[idx]\n        }\n\ndef extract_text_features(train_data, val_data, test_data):\n    # Create TF-IDF vectorizer with better parameters\n    vectorizer = TfidfVectorizer(\n        max_features=10000,  # Increased from 5000\n        ngram_range=(1, 2),  # Added bigrams\n        min_df=2,  # Remove very rare terms\n        max_df=0.95,  # Remove very common terms\n        strip_accents='unicode',\n        lowercase=True\n    )\n    \n    # Fit on training data only\n    X_train_text = vectorizer.fit_transform(train_data['transcriptions'])\n    X_val_text = vectorizer.transform(val_data['transcriptions'])\n    X_test_text = vectorizer.transform(test_data['transcriptions'])\n    \n    # Train SVM with better parameters\n    svm = LinearSVC(\n        C=1.0,\n        class_weight='balanced',\n        dual=False,\n        max_iter=2000,\n        random_state=42\n    )\n    svm.fit(X_train_text, train_data['labels'])\n    \n    # Get decision function scores\n    train_text_features = svm.decision_function(X_train_text)\n    val_text_features = svm.decision_function(X_val_text)\n    test_text_features = svm.decision_function(X_test_text)\n    \n    return (train_text_features.reshape(-1, 1), \n            val_text_features.reshape(-1, 1), \n            test_text_features.reshape(-1, 1))\n\ndef extract_image_features(loader, swin_model, feature_extractor, device):\n    features_list, labels_list = [], []\n    \n    swin_model.eval()\n    with torch.no_grad():\n        for batch in loader:\n            images = batch['image'].to(device)\n            labels = batch['label']\n            \n            # Get Swin features\n            outputs = swin_model(images)\n            \n            # Use pooled output instead of mean of last hidden state\n            features = outputs.pooler_output.cpu().numpy()\n            \n            features_list.append(features)\n            labels_list.extend(labels.numpy())\n    \n    return np.vstack(features_list), np.array(labels_list)\n\nclass MultimodalClassifier(nn.Module):\n    def __init__(self, text_feature_dim, image_feature_dim):\n        super().__init__()\n        \n        # Separate feature processing\n        self.text_processor = nn.Sequential(\n            nn.Linear(text_feature_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        \n        self.image_processor = nn.Sequential(\n            nn.Linear(image_feature_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        \n        # Fusion layers\n        self.fusion = nn.Sequential(\n            nn.Linear(256 + 512, 384),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(384, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 2)\n        )\n        \n        # Batch Normalization layers\n        self.bn_text = nn.BatchNorm1d(256)\n        self.bn_image = nn.BatchNorm1d(512)\n        self.bn_fusion = nn.BatchNorm1d(384)\n    \n    def forward(self, text_features, image_features):\n        # Process text features\n        text_features = self.text_processor(text_features)\n        text_features = self.bn_text(text_features)\n        \n        # Process image features\n        image_features = self.image_processor(image_features)\n        image_features = self.bn_image(image_features)\n        \n        # Combine features\n        combined = torch.cat((text_features, image_features), dim=1)\n        \n        # Final classification\n        output = self.fusion(combined)\n        return output\n\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    for batch in dataloader:\n        text_features = batch['text'].to(device)\n        image_features = batch['image'].to(device)\n        labels = batch['label'].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(text_features, image_features)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        _, preds = torch.max(outputs, 1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n    \n    metrics = precision_recall_fscore_support(\n        all_labels, all_preds, average='binary', zero_division=0\n    )\n    return total_loss / len(dataloader), metrics[2]  # Return loss and F1\n\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    total_loss = 0\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            text_features = batch['text'].to(device)\n            image_features = batch['image'].to(device)\n            labels = batch['label'].to(device)\n            \n            outputs = model(text_features, image_features)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            \n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    precision, recall, f1, _ = precision_recall_fscore_support(\n        all_labels, all_preds, average='binary', zero_division=0\n    )\n    return total_loss / len(dataloader), precision, recall, f1\n\ndef SVM_SwinTransformer(train_data, val_data, test_data, train_img_dir, val_img_dir, test_img_dir):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Create datasets with augmentation for training\n    train_dataset = MemeDataset(train_data, train_img_dir, is_training=True)\n    val_dataset = MemeDataset(val_data, val_img_dir, is_training=False)\n    test_dataset = MemeDataset(test_data, test_img_dir, is_training=False)\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=16)\n    test_loader = DataLoader(test_dataset, batch_size=16)\n    \n    # Initialize Swin Transformer\n    swin_model = SwinModel.from_pretrained(\"microsoft/swin-base-patch4-window12-384\").to(device)\n    feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-base-patch4-window12-384\")\n    \n    # Extract text features\n    train_text_features, val_text_features, test_text_features = extract_text_features(\n        train_data, val_data, test_data\n    )\n    \n    # Extract image features\n    train_image_features, train_labels = extract_image_features(train_loader, swin_model, feature_extractor, device)\n    val_image_features, val_labels = extract_image_features(val_loader, swin_model, feature_extractor, device)\n    test_image_features, test_labels = extract_image_features(test_loader, swin_model, feature_extractor, device)\n    \n    # Normalize features\n    scaler = StandardScaler()\n    train_text_features = scaler.fit_transform(train_text_features)\n    val_text_features = scaler.transform(val_text_features)\n    test_text_features = scaler.transform(test_text_features)\n    \n    scaler_img = StandardScaler()\n    train_image_features = scaler_img.fit_transform(train_image_features)\n    val_image_features = scaler_img.transform(val_image_features)\n    test_image_features = scaler_img.transform(test_image_features)\n\n    # Create combined datasets with processed features\n    train_combined_dataset = MultimodalDataset(train_text_features, train_image_features, train_labels)\n    val_combined_dataset = MultimodalDataset(val_text_features, val_image_features, val_labels)\n    test_combined_dataset = MultimodalDataset(test_text_features, test_image_features, test_labels)\n\n    # Create dataloaders for training\n    train_loader = DataLoader(train_combined_dataset, batch_size=16, shuffle=True)\n    val_loader = DataLoader(val_combined_dataset, batch_size=16)\n    test_loader = DataLoader(test_combined_dataset, batch_size=16)\n    \n    # Initialize model\n    model = MultimodalClassifier(\n        text_feature_dim=train_text_features.shape[1],\n        image_feature_dim=train_image_features.shape[1]\n    ).to(device)\n    \n    # Training settings\n    criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0]).to(device))  # Weight minority class more\n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n    \n    # Training loop\n    best_val_f1 = 0\n    epochs = 5\n    for epoch in range(epochs):\n        # Train\n        train_loss, train_f1 = train_epoch(model, train_loader, criterion, optimizer, device)\n        \n        # Validate\n        val_loss, val_prec, val_recall, val_f1 = evaluate(model, val_loader, criterion, device)\n        \n        # Learning rate scheduling\n        scheduler.step(val_f1)\n        \n        print(f\"Epoch {epoch+1}/{epochs}\")\n        print(f\"Train - Loss: {train_loss:.4f}, F1: {train_f1:.4f}\")\n        print(f\"Val - Loss: {val_loss:.4f}, F1: {val_f1:.4f}\")\n        \n        # Save best model\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            torch.save(model.state_dict(), 'best_model.pt')\n    \n    # Load best model and evaluate\n    model.load_state_dict(torch.load('best_model.pt'))\n    test_loss, test_prec, test_recall, test_f1 = evaluate(model, test_loader, criterion, device)\n    print(f\"\\nTest Results:\")\n    print(f\"Precision: {test_prec:.4f}\")\n    print(f\"Recall: {test_recall:.4f}\")\n    print(f\"F1 Score: {test_f1:.4f}\")\n    row_malayalam.append(['SVM','Swin','malayalam',f'{test_prec:.4f}',f'{test_recall:.4f}',f'{test_f1:.4f}'])\n\n# Run the model\nSVM_SwinTransformer(df_train_mal, df_val_mal, df_test_mal, train_img_dir_mal, val_img_dir_mal, test_img_dir_mal)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T15:21:30.473397Z","iopub.execute_input":"2025-01-27T15:21:30.473654Z","iopub.status.idle":"2025-01-27T15:22:18.252805Z","shell.execute_reply.started":"2025-01-27T15:21:30.473630Z","shell.execute_reply":"2025-01-27T15:22:18.251805Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\nTrain - Loss: 0.6804, F1: 0.5179\nVal - Loss: 0.6561, F1: 0.6292\nEpoch 2/5\nTrain - Loss: 0.6520, F1: 0.6046\nVal - Loss: 0.6276, F1: 0.6630\nEpoch 3/5\nTrain - Loss: 0.6289, F1: 0.6677\nVal - Loss: 0.5942, F1: 0.6667\nEpoch 4/5\nTrain - Loss: 0.5992, F1: 0.6897\nVal - Loss: 0.5601, F1: 0.7219\nEpoch 5/5\nTrain - Loss: 0.5766, F1: 0.6959\nVal - Loss: 0.5275, F1: 0.7595\n\nTest Results:\nPrecision: 0.6239\nRecall: 0.8718\nF1 Score: 0.7273\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# svm + vgg16 -> malayalam\nclass MemeDataset(Dataset):\n    def __init__(self, data, image_dir, transform=None):\n        self.data = data\n        self.image_dir = image_dir\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        img_name = f\"{self.image_dir}/{self.data.iloc[idx]['image_id']}.jpg\"\n        image = Image.open(img_name).convert('RGB')\n        image = self.transform(image)\n        \n        text = self.data.iloc[idx]['transcriptions']\n        label = self.data.iloc[idx]['labels']\n        \n        return image, text, label\n\ndef extract_text_features(train_data, test_data):\n    vectorizer = TfidfVectorizer(max_features=10000)\n    X_train_text = vectorizer.fit_transform(train_data['transcriptions'])\n    X_test_text = vectorizer.transform(test_data['transcriptions'])\n    \n    svm = LinearSVC(random_state=42)\n    svm.fit(X_train_text, train_data['labels'])\n    \n    train_text_features = svm.decision_function(X_train_text)\n    test_text_features = svm.decision_function(X_test_text)\n    \n    # Convert sparse matrices to dense NumPy arrays\n    return train_text_features.reshape(-1, 1), test_text_features.reshape(-1, 1)\n\ndef extract_image_features(loader):\n    # Load VGG16 model and remove the last layer\n    vgg16 = torchvision.models.vgg16(pretrained=True)\n    vgg16.classifier = nn.Sequential(*list(vgg16.classifier.children())[:-1])  # Remove last layer\n    vgg16.eval()\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    vgg16 = vgg16.to(device)\n    \n    all_features, all_labels = [], []\n    \n    with torch.no_grad():\n        for images, _, labels in loader:\n            images = images.to(device)\n            features = vgg16(images).cpu().numpy()\n            all_features.append(features)\n            all_labels.extend(labels.tolist())\n    \n    return np.vstack(all_features), np.array(all_labels)\n\nclass MultimodalClassifier(nn.Module):\n    def __init__(self, text_feature_dim, image_feature_dim):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(text_feature_dim + image_feature_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(64, 2)\n        )\n    \n    def forward(self, text_features, image_features):\n        combined = torch.cat((text_features, image_features), dim=1)\n        return self.fc(combined)\n\ndef train_multimodal_model(model, criterion, optimizer, text_features, image_features, labels):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    # Convert data to tensors and move to device\n    text_features = torch.FloatTensor(text_features).to(device)\n    image_features = torch.FloatTensor(image_features).to(device)\n    labels = torch.LongTensor(labels).to(device)\n    \n    model.train()\n    for epoch in range(5):\n        optimizer.zero_grad()\n        outputs = model(text_features, image_features)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        # Calculate training F1 score\n        _, predictions = torch.max(outputs, 1)\n        f1 = f1_score(labels.cpu().numpy(), predictions.cpu().numpy(), average='binary')\n        print(f\"Epoch [{epoch+1}/5], Loss: {loss.item():.4f}, F1 Score: {f1:.4f}\")\n\ndef evaluate_model(model, text_features, image_features, labels):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    # Convert data to tensors and move to device\n    text_features = torch.FloatTensor(text_features).to(device)\n    image_features = torch.FloatTensor(image_features).to(device)\n    labels = torch.LongTensor(labels).to(device)\n    \n    model.eval()\n    with torch.no_grad():\n        outputs = model(text_features, image_features)\n        _, predictions = torch.max(outputs, 1)\n    \n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels.cpu().numpy(), predictions.cpu().numpy(), average='binary'\n    )\n    \n    return precision, recall, f1\n\ndef SVM_VGG16():\n    # Load data\n    train_data = pd.read_csv(train_csv_mal)\n    test_data = pd.read_csv(test_csv_mal)\n    \n    # Prepare datasets and loaders\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    train_dataset = MemeDataset(train_data, train_img_dir_mal, transform)\n    test_dataset = MemeDataset(test_data, test_img_dir_mal, transform)\n    \n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=16)\n    \n    # Extract features\n    train_text_features, test_text_features = extract_text_features(train_data, test_data)\n    train_image_features, train_labels = extract_image_features(train_loader)\n    test_image_features, test_labels = extract_image_features(test_loader)\n\n    # Normalize features\n    scaler = StandardScaler()\n    train_text_features = scaler.fit_transform(train_text_features)\n    test_text_features = scaler.transform(test_text_features)\n    train_image_features = scaler.fit_transform(train_image_features)\n    test_image_features = scaler.transform(test_image_features)\n    \n    # Train multimodal model\n    model = MultimodalClassifier(\n        text_feature_dim=train_text_features.shape[1],\n        image_feature_dim=train_image_features.shape[1]\n    )\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n    train_multimodal_model(model, criterion, optimizer, train_text_features, train_image_features, train_labels)\n    \n    # Evaluate model\n    precision, recall, f1 = evaluate_model(model, test_text_features, test_image_features, test_labels)\n    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n    row_malayalam.append(['SVM','VGG16','malayalam',f'{precision:.4f}',f'{recall:.4f}',f'{f1:.4f}'])\n\nSVM_VGG16()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T15:22:18.254661Z","iopub.execute_input":"2025-01-27T15:22:18.254915Z","iopub.status.idle":"2025-01-27T15:22:40.518347Z","shell.execute_reply.started":"2025-01-27T15:22:18.254893Z","shell.execute_reply":"2025-01-27T15:22:40.517519Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/5], Loss: 0.7094, F1 Score: 0.4641\nEpoch [2/5], Loss: 0.6697, F1 Score: 0.5593\nEpoch [3/5], Loss: 0.6460, F1 Score: 0.5997\nEpoch [4/5], Loss: 0.5934, F1 Score: 0.6607\nEpoch [5/5], Loss: 0.5720, F1 Score: 0.7085\nPrecision: 0.6477, Recall: 0.7308, F1 Score: 0.6867\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(f\"\\nComparison Results for Malayalam:\")\nprint(tabulate(row_malayalam, headers=headers, tablefmt='grid'))\n\nresults_malayalam = pd.DataFrame(row_malayalam, columns=headers)\nresults_malayalam.to_csv(f'Comparison_Results_Malayalam.csv', index=False)\nprint(f\"\\nResults saved to 'Comparison_Results_for_Malayalam.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T15:32:27.142760Z","iopub.execute_input":"2025-01-27T15:32:27.143061Z","iopub.status.idle":"2025-01-27T15:32:27.150725Z","shell.execute_reply.started":"2025-01-27T15:32:27.143039Z","shell.execute_reply":"2025-01-27T15:32:27.149879Z"}},"outputs":[{"name":"stdout","text":"\nComparison Results for Malayalam:\n+--------------+---------------+------------+-------------+----------+------------+\n| Text Model   | Image Model   | Language   |   Precision |   Recall |   F1-Score |\n+==============+===============+============+=============+==========+============+\n| XLM-R        | VGG16         | malayalam  |      0.9016 |   0.7051 |     0.7914 |\n+--------------+---------------+------------+-------------+----------+------------+\n| XLM-R        | Swin          | malayalam  |      0.8312 |   0.8205 |     0.8258 |\n+--------------+---------------+------------+-------------+----------+------------+\n| SVM          | Swin          | malayalam  |      0.6239 |   0.8718 |     0.7273 |\n+--------------+---------------+------------+-------------+----------+------------+\n| SVM          | VGG16         | malayalam  |      0.6477 |   0.7308 |     0.6867 |\n+--------------+---------------+------------+-------------+----------+------------+\n\nResults saved to 'Comparison_Results_for_Malayalam.csv'\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"combined_rows = row_tamil + row_malayalam\nprint(f\"\\nComparison Results for both Tamil & Malayalam:\")\nprint(tabulate(combined_rows, headers=headers, tablefmt='grid'))\n\ncombined_results = pd.DataFrame(combined_rows, columns=headers)\ncombined_results.to_csv(f'Comparison_Results_Tamil_&_Malayalam.csv', index=False)\nprint(f\"\\nResults saved to 'Comparison_Results_for_Tamil_&_Malayalam.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T15:32:34.858494Z","iopub.execute_input":"2025-01-27T15:32:34.858778Z","iopub.status.idle":"2025-01-27T15:32:34.866746Z","shell.execute_reply.started":"2025-01-27T15:32:34.858757Z","shell.execute_reply":"2025-01-27T15:32:34.866040Z"}},"outputs":[{"name":"stdout","text":"\nComparison Results for both Tamil & Malayalam:\n+--------------+---------------+------------+-------------+----------+------------+\n| Text Model   | Image Model   | Language   |   Precision |   Recall |   F1-Score |\n+==============+===============+============+=============+==========+============+\n| SVM          | ResNet50      | tamil      |      0.3    |   0.9438 |     0.4553 |\n+--------------+---------------+------------+-------------+----------+------------+\n| SVM          | Swin          | tamil      |      0.302  |   0.6854 |     0.4192 |\n+--------------+---------------+------------+-------------+----------+------------+\n| mBERT        | ResNet50      | tamil      |      0.6596 |   0.6966 |     0.6776 |\n+--------------+---------------+------------+-------------+----------+------------+\n| mBERT        | Swin          | tamil      |      0.7463 |   0.5618 |     0.641  |\n+--------------+---------------+------------+-------------+----------+------------+\n| XLM-R        | VGG16         | malayalam  |      0.9016 |   0.7051 |     0.7914 |\n+--------------+---------------+------------+-------------+----------+------------+\n| XLM-R        | Swin          | malayalam  |      0.8312 |   0.8205 |     0.8258 |\n+--------------+---------------+------------+-------------+----------+------------+\n| SVM          | Swin          | malayalam  |      0.6239 |   0.8718 |     0.7273 |\n+--------------+---------------+------------+-------------+----------+------------+\n| SVM          | VGG16         | malayalam  |      0.6477 |   0.7308 |     0.6867 |\n+--------------+---------------+------------+-------------+----------+------------+\n\nResults saved to 'Comparison_Results_for_Tamil_&_Malayalam.csv'\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}